{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    !!! RT @mayasolovely: As a woman you shouldn't...\n",
       "1    !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
       "2    !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
       "3    !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
       "4    !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "#Loading raw data\n",
    "data_pd = pd.read_csv('C://Users/adele/Documents/GitHub/Automated-Hate-Speech-Detection/Dataset/labeled_data.csv',encoding = 'utf-8',sep=',',index_col=0)\n",
    "data_np = np.asarray(data_pd)\n",
    "                     \n",
    "tweets_pd_preclean = data_pd.iloc[:,5]\n",
    "\n",
    "tweets_pd_preclean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-43a592573ffc>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-43a592573ffc>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#data pre-processing\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def cleaning (tweets):\n",
    "    clean_data = []\n",
    "    for x in (text[:][0]): \n",
    "        # remove HTML tags\n",
    "        new_text = re.sub('<.*?>', '', x)   \n",
    "        # remove punctuations\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text)\n",
    "        # remove numbers\n",
    "        new_text = re.sub(r'\\d+','',new_text)\n",
    "        # lower case, .upper() for upper\n",
    "        new_text = new_text.lower()           \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_tweets\n",
    "\n",
    "def tokenization (self):\n",
    "    #Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    #and stems tweets. Returns a list of stemmed tokens.\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokenized_tweets\n",
    "\n",
    "def stopwordsremoved (tweet):\n",
    "    removed_tweets = [w for w in words if not w in stopwords]\n",
    "    return removed_tweets\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "此处还有tokenization 以及 stopwordsremoved\n",
    "不确定能不能想上面这么弄\n",
    "'''\n",
    "\n",
    "# Add cleaned data back into DataFrame\n",
    "data_pd['removed_tweets'] = removed_tweets\n",
    "\n",
    "# Remove temporary cleaned_text list (after transfer to DataFrame)\n",
    "del cleaned_text\n",
    "\n",
    "'''\n",
    "目前的困惑在于\n",
    "1）怎么往里面放CSV中的tweets\n",
    "2）而且为了应用 bag of words 是否一定要 lamentization. 我看不少别人的code其实没有用到 lema \n",
    "'''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature generation for baseline model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "'''\n",
    "首先的一个问题是 bag of words 如何与 Naiv Bayer 联系在一起\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline classifier using naive bayer\n",
    "# Import train_test_split function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "# 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(XXX.data, XXX.target, test_size=0.3,random_state=109) \n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "'''\n",
    "模型测评指数有待添加\n",
    "'''\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
